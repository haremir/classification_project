"""
Veri Ä°ÅŸleme ModÃ¼lÃ¼
==================

Bu modÃ¼l, phishing tespit projesi iÃ§in veri yÃ¼kleme, temizleme,
dÃ¶nÃ¼ÅŸtÃ¼rme ve hazÄ±rlama fonksiyonlarÄ±nÄ± iÃ§erir.

Fonksiyonlar:
    - load_arff_data: ARFF dosyasÄ±nÄ± yÃ¼kle
    - remove_duplicates: TekrarlÄ± satÄ±rlarÄ± kaldÄ±r
    - find_high_correlation_features: YÃ¼ksek korelasyonlu Ã¶zellik Ã§iftlerini bul
    - remove_multicollinear_features: Multicollinear Ã¶zellikleri Ã§Ä±kar
    - split_data: Train-test split yap
    - save_processed_data: Ä°ÅŸlenmiÅŸ veriyi kaydet
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, List, Set, Dict
from scipy.io import arff
from sklearn.model_selection import train_test_split
import warnings

warnings.filterwarnings('ignore')


def load_arff_data(file_path: Path) -> pd.DataFrame:
    """
    ARFF formatÄ±ndaki veri dosyasÄ±nÄ± yÃ¼kler ve pandas DataFrame'e Ã§evirir.
    
    Parameters:
        file_path (Path): ARFF dosyasÄ±nÄ±n yolu
        
    Returns:
        pd.DataFrame: YÃ¼klenmiÅŸ veri seti
        
    Raises:
        FileNotFoundError: Dosya bulunamazsa
        Exception: YÃ¼kleme hatasÄ±
    """
    if not file_path.exists():
        raise FileNotFoundError(f"Dosya bulunamadÄ±: {file_path}")
    
    try:
        # ARFF dosyasÄ±nÄ± yÃ¼kle
        data_arff, meta = arff.loadarff(file_path)
        df = pd.DataFrame(data_arff)
        
        # Byte string'leri decode et
        for col in df.columns:
            if df[col].dtype == object:
                try:
                    df[col] = df[col].str.decode('utf-8')
                except:
                    pass
        
        # Veri tiplerini integer'a Ã§evir
        df = df.astype(int)
        
        print(f"âœ… ARFF dosyasÄ± baÅŸarÄ±yla yÃ¼klendi: {file_path.name}")
        print(f"   Boyut: {df.shape[0]:,} satÄ±r x {df.shape[1]} sÃ¼tun")
        
        return df
        
    except Exception as e:
        print(f"âŒ YÃ¼kleme hatasÄ±: {e}")
        raise


def remove_duplicates(df: pd.DataFrame, keep: str = 'first', 
                     verbose: bool = True) -> pd.DataFrame:
    """
    Veri setindeki tekrarlÄ± satÄ±rlarÄ± kaldÄ±rÄ±r.
    
    Parameters:
        df (pd.DataFrame): Veri seti
        keep (str): Hangi tekrarÄ± tutacaÄŸÄ±z ('first', 'last', False)
        verbose (bool): Ä°ÅŸlem bilgilerini yazdÄ±r
        
    Returns:
        pd.DataFrame: TemizlenmiÅŸ veri seti
    """
    original_shape = df.shape[0]
    duplicate_count = df.duplicated().sum()
    
    if verbose:
        print(f"\n{'='*80}")
        print("TEKRARLI SATIRLARI TEMÄ°ZLEME")
        print(f"{'='*80}")
        print(f"\nğŸ“Š Ã–nceki Durum:")
        print(f"   â€¢ Toplam SatÄ±r: {original_shape:,}")
        print(f"   â€¢ TekrarlÄ± SatÄ±r: {duplicate_count:,}")
        print(f"   â€¢ Benzersiz SatÄ±r: {original_shape - duplicate_count:,}")
    
    # TekrarlarÄ± kaldÄ±r
    df_clean = df.drop_duplicates(keep=keep)
    new_shape = df_clean.shape[0]
    removed_count = original_shape - new_shape
    
    if verbose:
        print(f"\nğŸ“Š Sonraki Durum:")
        print(f"   â€¢ Toplam SatÄ±r: {new_shape:,}")
        print(f"   â€¢ KaldÄ±rÄ±lan SatÄ±r: {removed_count:,}")
        print(f"   â€¢ Veri KaybÄ±: {(removed_count / original_shape * 100):.2f}%")
        print(f"\nâœ… Temizleme tamamlandÄ±!")
    
    return df_clean


def find_high_correlation_features(df: pd.DataFrame, 
                                   feature_names: List[str],
                                   target_name: str,
                                   threshold: float = 0.8,
                                   verbose: bool = True) -> Set[str]:
    """
    YÃ¼ksek korelasyonlu Ã¶zellik Ã§iftlerini bulur ve hedef deÄŸiÅŸkenle
    korelasyonu dÃ¼ÅŸÃ¼k olanlarÄ± Ã§Ä±karÄ±lacak liste olarak dÃ¶ner.
    
    Parameters:
        df (pd.DataFrame): Veri seti
        feature_names (List[str]): Ã–zellik isimleri listesi
        target_name (str): Hedef deÄŸiÅŸken ismi
        threshold (float): Korelasyon eÅŸiÄŸi (varsayÄ±lan: 0.8)
        verbose (bool): Ä°ÅŸlem bilgilerini yazdÄ±r
        
    Returns:
        Set[str]: Ã‡Ä±karÄ±lacak Ã¶zellikler kÃ¼mesi
    """
    # Sadece Ã¶zelliklerin korelasyon matrisi
    X = df[feature_names]
    corr_matrix = X.corr()
    
    # YÃ¼ksek korelasyonlu Ã§iftleri bul
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) >= threshold:
                high_corr_pairs.append({
                    'Ã–zellik 1': corr_matrix.columns[i],
                    'Ã–zellik 2': corr_matrix.columns[j],
                    'Korelasyon': corr_matrix.iloc[i, j]
                })
    
    if verbose:
        print(f"\n{'='*80}")
        print("YÃœKSEK KORELASYONLU Ã–ZELLÄ°KLERÄ°N TESPÄ°TÄ°")
        print(f"{'='*80}")
    
    if len(high_corr_pairs) == 0:
        if verbose:
            print(f"\nâœ… {threshold} eÅŸiÄŸinde yÃ¼ksek korelasyonlu Ã¶zellik Ã§ifti bulunamadÄ±!")
        return set()
    
    if verbose:
        print(f"\nâš ï¸  {len(high_corr_pairs)} yÃ¼ksek korelasyonlu Ã§ift tespit edildi (|r| â‰¥ {threshold}):")
        for pair in high_corr_pairs:
            print(f"   â€¢ {pair['Ã–zellik 1']} â†” {pair['Ã–zellik 2']}: {pair['Korelasyon']:.4f}")
    
    # Hedef deÄŸiÅŸkenle korelasyonlarÄ± hesapla
    target_corr = df[feature_names + [target_name]].corr()[target_name].drop(target_name)
    
    # Her Ã§iftten hedef ile korelasyonu dÃ¼ÅŸÃ¼k olanÄ± seÃ§
    features_to_drop = set()
    
    if verbose:
        print(f"\nğŸ“‹ Ã‡Ä±karÄ±lacak Ã–zelliklerin Belirlenmesi:")
        print("   (Her Ã§iftten hedef deÄŸiÅŸkenle korelasyonu dÃ¼ÅŸÃ¼k olanÄ± Ã§Ä±karÄ±lacak)")
        print(f"\n   {'Ã–zellik 1':<30} {'r(target)':<12} {'Ã–zellik 2':<30} {'r(target)':<12} {'Ã‡Ä±karÄ±lacak':<30}")
        print(f"   {'-'*115}")
    
    for pair in high_corr_pairs:
        feat1 = pair['Ã–zellik 1']
        feat2 = pair['Ã–zellik 2']
        corr1 = abs(target_corr[feat1])
        corr2 = abs(target_corr[feat2])
        
        # Hedef ile korelasyonu dÃ¼ÅŸÃ¼k olanÄ± Ã§Ä±kar
        to_drop = feat1 if corr1 < corr2 else feat2
        features_to_drop.add(to_drop)
        
        if verbose:
            print(f"   {feat1:<30} {corr1:<12.4f} {feat2:<30} {corr2:<12.4f} {to_drop:<30}")
    
    if verbose:
        print(f"\nâœ… Ã‡Ä±karÄ±lacak Ã–zellikler ({len(features_to_drop)} adet):")
        for feat in sorted(features_to_drop):
            print(f"   â€¢ {feat} (hedef korelasyon: {abs(target_corr[feat]):.4f})")
    
    return features_to_drop


def remove_multicollinear_features(df: pd.DataFrame,
                                   feature_names: List[str],
                                   target_name: str,
                                   threshold: float = 0.8,
                                   verbose: bool = True) -> Tuple[pd.DataFrame, List[str]]:
    """
    Multicollinear Ã¶zellikleri tespit edip Ã§Ä±karÄ±r.
    
    Parameters:
        df (pd.DataFrame): Veri seti
        feature_names (List[str]): Ã–zellik isimleri listesi
        target_name (str): Hedef deÄŸiÅŸken ismi
        threshold (float): Korelasyon eÅŸiÄŸi
        verbose (bool): Ä°ÅŸlem bilgilerini yazdÄ±r
        
    Returns:
        Tuple[pd.DataFrame, List[str]]: (TemizlenmiÅŸ veri, kalan Ã¶zellikler listesi)
    """
    # Ã‡Ä±karÄ±lacak Ã¶zellikleri bul
    features_to_drop = find_high_correlation_features(
        df, feature_names, target_name, threshold, verbose
    )
    
    # Kalan Ã¶zellikler
    remaining_features = [feat for feat in feature_names if feat not in features_to_drop]
    
    # Veri setini gÃ¼ncelle
    df_cleaned = df[remaining_features + [target_name]].copy()
    
    if verbose:
        print(f"\n{'='*80}")
        print("Ã–ZET")
        print(f"{'='*80}")
        print(f"   â€¢ BaÅŸlangÄ±Ã§ Ã–zellik SayÄ±sÄ±: {len(feature_names)}")
        print(f"   â€¢ Ã‡Ä±karÄ±lan Ã–zellik: {len(features_to_drop)}")
        print(f"   â€¢ Kalan Ã–zellik: {len(remaining_features)}")
        print(f"\nâœ… Multicollinearity temizleme tamamlandÄ±!")
    
    return df_cleaned, remaining_features


def split_data(df: pd.DataFrame,
              feature_names: List[str],
              target_name: str,
              test_size: float = 0.2,
              random_state: int = 42,
              stratify: bool = True,
              verbose: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame, 
                                             pd.Series, pd.Series]:
    """
    Veri setini train ve test olarak bÃ¶ler.
    
    Parameters:
        df (pd.DataFrame): Veri seti
        feature_names (List[str]): Ã–zellik isimleri listesi
        target_name (str): Hedef deÄŸiÅŸken ismi
        test_size (float): Test seti oranÄ± (varsayÄ±lan: 0.2)
        random_state (int): Random seed (tekrarlanabilirlik iÃ§in)
        stratify (bool): SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± koru
        verbose (bool): Ä°ÅŸlem bilgilerini yazdÄ±r
        
    Returns:
        Tuple: (X_train, X_test, y_train, y_test)
    """
    X = df[feature_names].copy()
    y = df[target_name].copy()
    
    if verbose:
        print(f"\n{'='*80}")
        print("VERÄ° BÃ–LME (TRAIN-TEST SPLIT)")
        print(f"{'='*80}")
        print(f"\nğŸ“Š Veri Seti Bilgileri:")
        print(f"   â€¢ Toplam Veri: {len(X):,} satÄ±r")
        print(f"   â€¢ Ã–zellik SayÄ±sÄ±: {X.shape[1]}")
        print(f"   â€¢ Hedef DeÄŸiÅŸken: {target_name}")
        print(f"\nâš™ï¸  Split Parametreleri:")
        print(f"   â€¢ Train: %{(1-test_size)*100:.0f}")
        print(f"   â€¢ Test: %{test_size*100:.0f}")
        print(f"   â€¢ Random State: {random_state}")
        print(f"   â€¢ Stratified: {'Evet' if stratify else 'HayÄ±r'}")
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y if stratify else None
    )
    
    if verbose:
        print(f"\nâœ… Split tamamlandÄ±!")
        print(f"\nğŸ“Š Train Set:")
        print(f"   â€¢ Boyut: {X_train.shape[0]:,} satÄ±r x {X_train.shape[1]} sÃ¼tun")
        train_class_dist = y_train.value_counts().sort_index()
        for cls, count in train_class_dist.items():
            percentage = (count / len(y_train)) * 100
            print(f"   â€¢ SÄ±nÄ±f {cls:2}: {count:,} ({percentage:.2f}%)")
        
        print(f"\nğŸ“Š Test Set:")
        print(f"   â€¢ Boyut: {X_test.shape[0]:,} satÄ±r x {X_test.shape[1]} sÃ¼tun")
        test_class_dist = y_test.value_counts().sort_index()
        for cls, count in test_class_dist.items():
            percentage = (count / len(y_test)) * 100
            print(f"   â€¢ SÄ±nÄ±f {cls:2}: {count:,} ({percentage:.2f}%)")
    
    return X_train, X_test, y_train, y_test


def save_processed_data(X_train: pd.DataFrame,
                       X_test: pd.DataFrame,
                       y_train: pd.Series,
                       y_test: pd.Series,
                       feature_names: List[str],
                       target_name: str,
                       train_path: Path,
                       test_path: Path,
                       feature_path: Path = None,
                       verbose: bool = True) -> None:
    """
    Ä°ÅŸlenmiÅŸ train ve test verilerini diske kaydeder.
    
    Parameters:
        X_train (pd.DataFrame): Train Ã¶zellikleri
        X_test (pd.DataFrame): Test Ã¶zellikleri
        y_train (pd.Series): Train hedef deÄŸiÅŸken
        y_test (pd.Series): Test hedef deÄŸiÅŸken
        feature_names (List[str]): Ã–zellik isimleri listesi
        target_name (str): Hedef deÄŸiÅŸken ismi
        train_path (Path): Train dosyasÄ± kayÄ±t yolu
        test_path (Path): Test dosyasÄ± kayÄ±t yolu
        feature_path (Path): Ã–zellik listesi kayÄ±t yolu (opsiyonel)
        verbose (bool): Ä°ÅŸlem bilgilerini yazdÄ±r
    """
    if verbose:
        print(f"\n{'='*80}")
        print("Ä°ÅLENMÄ°Å VERÄ°YÄ° KAYDETME")
        print(f"{'='*80}")
    
    # Dizinleri oluÅŸtur
    train_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Train ve test'i birleÅŸtir (hedef dahil)
    train_out = pd.concat([X_train, y_train.rename(target_name)], axis=1)
    test_out = pd.concat([X_test, y_test.rename(target_name)], axis=1)
    
    # Kaydet
    train_out.to_csv(train_path, index=False)
    test_out.to_csv(test_path, index=False)
    
    if verbose:
        print(f"\nâœ… Veri dosyalarÄ± kaydedildi:")
        print(f"   â€¢ Train: {train_path}")
        print(f"   â€¢ Test:  {test_path}")
        print(f"\nğŸ“Š Dosya BoyutlarÄ±:")
        print(f"   â€¢ Train: {train_out.shape[0]:,} satÄ±r x {train_out.shape[1]} sÃ¼tun")
        print(f"   â€¢ Test:  {test_out.shape[0]:,} satÄ±r x {test_out.shape[1]} sÃ¼tun")
    
    # Ã–zellik listesini kaydet (opsiyonel)
    if feature_path is not None:
        with open(feature_path, 'w', encoding='utf-8') as f:
            for feat in feature_names:
                f.write(feat + '\n')
        if verbose:
            print(f"   â€¢ Ã–zellikler: {feature_path}")
            print(f"   â€¢ Ã–zellik SayÄ±sÄ±: {len(feature_names)}")
    
    if verbose:
        print(f"\nâœ… TÃ¼m dosyalar baÅŸarÄ±yla kaydedildi!")


def get_data_summary(df: pd.DataFrame, 
                    feature_names: List[str],
                    target_name: str) -> Dict:
    """
    Veri seti hakkÄ±nda Ã¶zet bilgi dÃ¶ner.
    
    Parameters:
        df (pd.DataFrame): Veri seti
        feature_names (List[str]): Ã–zellik isimleri
        target_name (str): Hedef deÄŸiÅŸken ismi
        
    Returns:
        Dict: Ã–zet bilgiler
    """
    summary = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'feature_count': len(feature_names),
        'target_name': target_name,
        'missing_values': df.isnull().sum().sum(),
        'duplicate_rows': df.duplicated().sum(),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'class_distribution': df[target_name].value_counts().to_dict(),
        'class_balance_ratio': df[target_name].value_counts().max() / df[target_name].value_counts().min()
    }
    return summary


def print_data_summary(summary: Dict, verbose: bool = True) -> None:
    """
    Veri Ã¶zeti bilgilerini yazdÄ±rÄ±r.
    
    Parameters:
        summary (Dict): Ã–zet bilgiler
        verbose (bool): DetaylÄ± yazdÄ±rma
    """
    if not verbose:
        return
    
    print(f"\n{'='*80}")
    print("VERÄ° SETÄ° Ã–ZETÄ°")
    print(f"{'='*80}")
    print(f"\nğŸ“Š Temel Bilgiler:")
    print(f"   â€¢ Toplam SatÄ±r: {summary['total_rows']:,}")
    print(f"   â€¢ Toplam SÃ¼tun: {summary['total_columns']}")
    print(f"   â€¢ Ã–zellik SayÄ±sÄ±: {summary['feature_count']}")
    print(f"   â€¢ Hedef DeÄŸiÅŸken: {summary['target_name']}")
    
    print(f"\nğŸ” Veri Kalitesi:")
    print(f"   â€¢ Eksik DeÄŸer: {summary['missing_values']:,}")
    print(f"   â€¢ TekrarlÄ± SatÄ±r: {summary['duplicate_rows']:,}")
    print(f"   â€¢ Bellek KullanÄ±mÄ±: {summary['memory_usage_mb']:.2f} MB")
    
    print(f"\nğŸ“ˆ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
    for cls, count in sorted(summary['class_distribution'].items()):
        percentage = (count / summary['total_rows']) * 100
        print(f"   â€¢ SÄ±nÄ±f {cls:2}: {count:,} ({percentage:.2f}%)")
    
    print(f"\nâš–ï¸  Dengesizlik OranÄ±: {summary['class_balance_ratio']:.2f}:1")
    print(f"{'='*80}\n")